 -*- coding: utf-8 -*-
"""project3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J6kqkkE3OHf8eqndyAYYQtzEgEh4Khh9
"""

pip install tensorflow matplotlib pillow

from google.colab import drive
drive.mount('/content/drive')

data_directory = "/content/drive/MyDrive/dataset_proj/"



!pip install tensorflow matplotlib pillow

data_directory = "/absolute/path/to/your/dataset"

!ls "/content/drive/MyDrive/dataset"

!ls "/content/drive/MyDrive/"

from google.colab import drive
drive.mount('/content/drive')

!ls "/content/drive/MyDrive"

!ls "/content/drive/MyDrive/dataset_proj"

data_directory = "/content/drive/MyDrive/dataset_proj/"

!ls "/content/drive/MyDrive/dataset_proj/"

# Importing LoRA components
from tensorflow.keras.initializers import RandomNormal

#LoRA (Low-Rank Adaptation) Layer
#The layer consists of:
#lora_A (a smaller matrix reducing dimensionality).
#lora_B (reconstructing useful features).
#This helps in training models efficiently without sacrificing accuracy.
# LoRA Layer Definition
class LoRALayer(tf.keras.layers.Layer):
    def __init__(self, units, rank=4, **kwargs):
        super(LoRALayer, self).__init__(**kwargs)
        self.units = units
        self.rank = rank

    def build(self, input_shape):
        self.kernel = self.add_weight(name='kernel',
                                     shape=(input_shape[-1], self.units),
                                     initializer='glorot_uniform',
                                     trainable=True)
        self.lora_A = self.add_weight(name='lora_A',
                                      shape=(input_shape[-1], self.rank),
                                      initializer=RandomNormal(mean=0.0, stddev=0.01),
                                      trainable=True)
        self.lora_B = self.add_weight(name='lora_B',
                                      shape=(self.rank, self.units),
                                      initializer=RandomNormal(mean=0.0, stddev=0.01),
                                      trainable=True)

    def call(self, inputs):
        base_output = tf.matmul(inputs, self.kernel)
        lora_output = tf.matmul(tf.matmul(inputs, self.lora_A), self.lora_B)
        return base_output + lora_output

    def compute_output_shape(self, input_shape):
        return (input_shape[0], self.units)


#Building the CNN Model with LoRA
#Conv2D (Convolutional Layer): Extracts important features like edges and patterns from images using filters.
#ReLU (Activation Function): Converts negative values to zero, helping the model learn efficiently.
#MaxPooling2D (Pooling Layer): Reduces image size while keeping essential features, preventing overfitting.
#Flatten (Reshaping Layer): Converts 2D feature maps into a 1D vector for the dense layer.
#Dense (Fully Connected Layer): Makes predictions by combining extracted features.
#Softmax (Output Layer): Converts final values into probabilities for multi-class classification.
input_layer = Input(shape=(128, 128, 3))
conv1 = Conv2D(32, (3, 3), activation='relu')(input_layer)
pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
conv2 = Conv2D(64, (3, 3), activation='relu')(pool1)
pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)
conv3 = Conv2D(128, (3, 3), activation='relu')(pool2)
pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)
flat = Flatten()(pool3)
dense1 = LoRALayer(512)(flat)

# Applying LoRA here
output_layer = Dense(15, activation='softmax')(dense1)

model = Model(inputs=input_layer, outputs=output_layer)

model.compile(optimizer=Adam(learning_rate=0.00005),  # Reduce LR
              loss='categorical_crossentropy',
              metrics=['accuracy'])


from tensorflow.keras.utils import plot_model
plot_model(model, to_file='model_architecture.png', show_shapes=True)
model.summary()

# Data Preparation
#The images are resized to 128x128 pixels.
#Data augmentation techniques like rotation, brightness adjustment, and flipping are used to make the model robust.
#The dataset is split into training (80%) and validation (20%) subsets.
image_size = (128, 128)
batch_size = 32
train_datagen = ImageDataGenerator(
    rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    rotation_range=20,  # New
    brightness_range=[0.8, 1.2],  # New
    validation_split=0.2
)

# **Verify and Correct this path**
#Checking the Dataset Directory
#The script prints the dataset folder’s contents to ensure the correct path is used.
#If the dataset directory is not found, an error is raised to alert the user.
data_directory = '/content/drive/MyDrive/dataset_proj/'
# Print the contents to check the directory structure
print(os.listdir(data_directory))

# Ensure the directory exists
if not os.path.exists(data_directory):
    raise FileNotFoundError(f"Dataset directory '{data_directory}' not found. Please check the path.")

# Create the generators
#Creating Training and Validation Data Generators
#The images are loaded from the directory.
#They are rescaled (1/255.0) to normalize pixel values.
#The model is trained on training data, while validation data is used to check accuracy.
train_generator = train_datagen.flow_from_directory(
    data_directory,
    target_size=image_size,
    batch_size=batch_size,
    class_mode='categorical',  # <-- Ensure it's categorical
    subset='training'
)

validation_generator = train_datagen.flow_from_directory(
    data_directory,
    target_size=image_size,
    batch_size=batch_size,
    class_mode='categorical',  # <-- Ensure it's categorical
    subset='validation'
)

# Training the Model
#Early stopping is applied to stop training if validation loss doesn’t improve for 5 consecutive epochs.
#The best model is saved using ModelCheckpoint.
#The model is trained for 30 epochs using the dataset.

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
    ModelCheckpoint('best_model.keras', save_best_only=True)  # Save in recommended format
]

history = model.fit(
    train_generator,
    epochs=30,
    validation_data=validation_generator
)

# Plotting Training Results
#Plotting Accuracy Graphs
#The training and validation accuracy trends are plotted to observe performance.
#Helps to identify overfitting or underfitting.
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

model.save("/content/drive/MyDrive/Minor Project/skin_spot_detector_model.h5")

from tensorflow.keras.models import load_model

# Define your LoRALayer class here (same as in your training script)

# LoRA Layer Definition
class LoRALayer(tf.keras.layers.Layer):
    def __init__(self, units, rank=4, **kwargs):
        super(LoRALayer, self).__init__(**kwargs)
        self.units = units
        self.rank = rank

    def build(self, input_shape):
        self.kernel = self.add_weight(name='kernel',
                                     shape=(input_shape[-1], self.units),
                                     initializer='glorot_uniform',
                                     trainable=True)
        self.lora_A = self.add_weight(name='lora_A',
                                      shape=(input_shape[-1], self.rank),
                                      initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),
                                      trainable=True)
        self.lora_B = self.add_weight(name='lora_B',
                                      shape=(self.rank, self.units),
                                      initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),
                                      trainable=True)

    def call(self, inputs):
        base_output = tf.matmul(inputs, self.kernel)
        lora_output = tf.matmul(tf.matmul(inputs, self.lora_A), self.lora_B)
        return base_output + lora_output

    def compute_output_shape(self, input_shape):
        return (input_shape[0], self.units)


# Load the model with custom_objects
model = load_model("/content/drive/MyDrive/Minor Project/skin_spot_detector_model.h5",
                   custom_objects={'LoRALayer': LoRALayer})

from tensorflow.keras.preprocessing import image
import numpy as np
import os
from tensorflow.keras.models import load_model
from tensorflow.keras.optimizers import Adam

# ... (LoRALayer definition if necessary) ...

model = load_model("/content/drive/MyDrive/Minor Project/skin_spot_detector_model.h5",
                   custom_objects={'LoRALayer': LoRALayer})

# Recompile the model to avoid the warning
model.compile(optimizer=Adam(learning_rate=0.00005),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Update the image path to point to a specific image file
# Assuming there's an image file named 'image1.jpg' in the 'acne' directory
test_image_path = "/content/drive/MyDrive/dataset_proj/acne/acne_1.jpg"

# Load and preprocess the image
img = image.load_img(test_image_path, target_size=(128, 128))
img_array = image.img_to_array(img) / 255.0
img_array = np.expand_dims(img_array, axis=0)

# Make prediction
prediction = model.predict(img_array)

# Interpret the prediction
# Assuming you have 15 classes, this will give you the predicted class index
predicted_class_index = np.argmax(prediction)

# Get the class labels from your training generator (if you used one)
class_labels = list(train_generator.class_indices.keys())  # Replace 'train_generator' if necessary

# Print the predicted class
predicted_class = class_labels[predicted_class_index]
print(f"Predicted class: {predicted_class}")

# Optionally, print the prediction probabilities for each class:
print("Prediction probabilities:")
for i, prob in enumerate(prediction[0]):
    print(f"{class_labels[i]}: {prob:.4f}")